{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47494233",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m#Pre Req, in linux type these commands in console \u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m# wget http://nlp.stanford.edu/data/glove.6B.zip\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# unzip glove.6B.zip\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m      8\u001b[0m embeddings_index \u001b[39m=\u001b[39m {}\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "#Pre Req, in linux type these commands in console \n",
    "# wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "# unzip glove.6B.zip\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "embeddings_index = {}\n",
    "with open('glove.6B.100d.txt') as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "     \n",
    "fixed_embedding_matrix = np.zeros((vocab_size, 100))\n",
    "for i, word in enumerate(vocabulary):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        fixed_embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "fixed_embedding = tf.keras.layers.Embedding(\n",
    "    self.nli_voba_size,\n",
    "    100,\n",
    "    embeddings_initializer=tf.keras.initializers.Constant(fixed_embedding_matrix),\n",
    "    trainable=False,\n",
    "    mask_zero=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34afda83",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-cd47dc98b6e1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mNl2SqlTranslator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnl_text_processor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msql_text_processor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfixed_embedding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[1;31m# Natural language\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnl_text_processor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnl_text_processor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "class Nl2SqlTranslator(tf.keras.Model):\n",
    "    def __init__(self, nl_text_processor, sql_text_processor, fixed_embedding, unit=128):\n",
    "        super().__init__()\n",
    "        # Natural language\n",
    "        self.nl_text_processor = nl_text_processor\n",
    "        self.nl_voba_size = len(nl_text_processor.get_vocabulary())\n",
    "        self.nl_embedding = tf.keras.layers.Embedding(\n",
    "            self.nl_voba_size,\n",
    "            output_dim=unit,\n",
    "            mask_zero=True)\n",
    "        self.fixed_embedding = fixed_embedding\n",
    "        self.nl_rnn = tf.keras.layers.Bidirectional(layer=tf.keras.layers.LSTM(int(unit/2), return_sequences=True, return_state=True))\n",
    "        # Attention\n",
    "        self.attention = tf.keras.layers.Attention()\n",
    "        # SQL\n",
    "        self.sql_text_processor = sql_text_processor\n",
    "        self.sql_voba_size = len(sql_text_processor.get_vocabulary())\n",
    "        self.sql_embedding = tf.keras.layers.Embedding(\n",
    "            self.sql_voba_size,\n",
    "            output_dim=unit,\n",
    "            mask_zero=True)\n",
    "        self.sql_rnn = tf.keras.layers.LSTM(unit, return_sequences=True, return_state=True)\n",
    "        # Output\n",
    "        self.out = tf.keras.layers.Dense(self.sql_voba_size)\n",
    "            \n",
    "    def call(self, nl_text, sql_text, training=True):\n",
    "        nl_tokens = self.nl_text_processor(nl_text) # Shape: (batch, Ts)\n",
    "        nl_vectors = self.nl_embedding(nl_tokens, training=training) # Shape: (batch, Ts, embedding_dim)\n",
    "        nl_fixed_vectors = self.fixed_embedding(nl_tokens) # Shape: (batch, Ts, 100)\n",
    "        nl_combined_vectors = tf.concat([nl_vectors, nl_fixed_vectors], -1) # Shape: (batch, Ts, embedding_dim+100)\n",
    "        nl_rnn_out, fhstate, fcstate, bhstate, bcstate = self.nl_rnn(nl_vectors, training=training) # Shape: (batch, Ts, bi_rnn_output_dim), (batch, rnn_output_dim) ...\n",
    "        nl_hstate = tf.concat([fhstate, bhstate], -1)\n",
    "        nl_cstate = tf.concat([fcstate, bcstate], -1)\n",
    "        \n",
    "        sql_tokens = self.sql_text_processor(sql_text) # Shape: (batch, Te)\n",
    "        expected = sql_tokens[:,1:] # Shape: (batch, Te-1)\n",
    "        \n",
    "        teacher_forcing = sql_tokens[:,:-1] # Shape: (batch, Te-1)\n",
    "        sql_vectors = self.sql_embedding(teacher_forcing, training=training) # Shape: (batch, Te-1, embedding_dim)\n",
    "        sql_in = self.attention(inputs=[sql_vectors,nl_rnn_out], mask=[sql_vectors._keras_mask, nl_rnn_out._keras_mask], training=training)\n",
    "        \n",
    "        trans_vectors, _, _ = self.sql_rnn(sql_in, initial_state=[nl_hstate, nl_cstate], training=training) # Shape: (batch, Te-1, rnn_output_dim)\n",
    "        out = self.out(trans_vectors, training=training) # Shape: (batch, Te-1, sql_vocab_size)\n",
    "        return out, expected, out._keras_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92b7705-0d91-4b9d-90b9-97b3ee428bcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
